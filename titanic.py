# -*- coding: utf-8 -*-
"""Titanic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FjwZ2jPhNJxPRdpLbsfNmSDrsHWsqnXu
"""

from urllib.request import urlretrieve
url = "https://github.com/Elwing-Chou/ml1216/raw/main/train.csv"
urlretrieve(url,"train.csv")
url = "https://github.com/Elwing-Chou/ml1216/raw/main/test.csv"
urlretrieve(url,"test.csv")

import pandas as pd
train = pd.read_csv("train.csv", encoding = "utf-8")
test = pd.read_csv("test.csv", encoding = "utf-8")

#填補缺失值
#use as many data as possible

#把train & test合在一起
datas = pd.concat([train, test],axis = 0, ignore_index=True) 
datas = datas.drop(["PassengerId","Survived"], axis = 1)
datas

#看那些欄位有空值
s = datas.isna().sum() #空值回傳True
#series[跟資料筆數一樣多的true/false]
s[s > 0].sort_values(ascending = False)

#處理 ticket
#算出有哪些人共用ticket
count = datas["Ticket"].value_counts()
def share(tn):
    return count[tn]
datas["Ticket"]=datas["Ticket"].apply(share)

#處理Pclass
#取中位數
med = datas.median().drop("Pclass") #小心有類別變數，drop("Pclass")

#把空值全部填入中位數
datas = datas.fillna(med)

#處理Embarked
#用most填入空值
most = datas["Embarked"].value_counts().idxmax()
datas["Embarked"] = datas["Embarked"].fillna(most)

#處理 Cabin 
# let空值be None
def cabin(c):
    if pd.isna(c):
        return None
    else:
        return c[0]
datas["Cabin"] = datas["Cabin"].apply(cabin)

#處理name，目標是取出稱謂
def namecut(n):
    n = n.split(",")[-1].split(".")[0]
    return n.strip() #strip:丟掉左右兩邊空白
count = datas["Name"].apply(namecut).value_counts()
# ['Mr', 'Miss', 'Mrs', 'Master']
reserved = count[:4].index

def namecut(n):
    n = n.split(",")[-1].split(".")[0]
    n = n.strip()
    if n in reserved:
        return n
    else:
        return None
datas["Name"] = datas["Name"].apply(namecut)

datas

#對Pclass編碼
datas = pd.get_dummies(datas)
datas = pd.get_dummies(datas,columns=["Pclass"])
datas

#Sibsp, Parch, 多給一個[合在一起]的選擇(可增加欄位，不要刪除欄位)
datas["Family"] = datas["SibSp"] + datas["Parch"]
datas

#組合式演算法(ensemble):多個分類器
#iloc第幾列
#iloc:[1st,2nd,...]
x_train = datas.iloc[:len(train)]
y_train = train["Survived"]
x_test = datas.iloc[len(train):]
x_test

# A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve
#cv = 10，分為十個隨機森林，每個吃不同資料進去訓練
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
params = {
    "n_estimators":range(15, 35),
    "max_depth":range(5, 9)
}
clf = RandomForestClassifier() #模型本人
search = GridSearchCV(clf,params, n_jobs = -1, cv = 10)
search.fit(x_train, y_train)
print(search.best_score_)
print(search.best_params_)

#cross_val_score哪個森林表現好?
import numpy as np
from sklearn.model_selection import cross_val_score
clf = RandomForestClassifier(n_estimators = 25, max_depth  = 7)
scores = cross_val_score(clf,x_train,y_train,n_jobs = -1,cv= 5)
print("scores:",scores)
print("Avg score:",np.average(scores))

clf = RandomForestClassifier(n_estimators = 25, max_depth  = 7)
clf.fit(x_train,y_train)
pre = clf.predict(x_test)
df = pd.DataFrame({
    "PassengerId":test["PassengerId"],
    "Survived":pre
})
df.to_csv("rf.csv", encoding="utf-8", index=False)

from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
print(len(clf.estimators_))

plt.figure(figsize = (15,15)) #控圖大小 in inch
tree = plot_tree(clf.estimators_[0], 
          feature_names = datas.columns,
          class_names = ["Dead","Alive"],
          max_depth = 2,
          filled =True)
plt.show()

from sklearn.preprocessing import MinMaxScaler
# iloc: [1st, 2nd, ....]
scaler = MinMaxScaler()
datas_scale = scaler.fit_transform(datas)
datas_scale = pd.DataFrame(datas_scale, columns=datas.columns)
x_train_scale = datas_scale.iloc[:len(train)]
x_test_scale = datas_scale.iloc[len(train):]
x_test_scale

from sklearn.neighbors import KNeighborsClassifier
params = {
   "n_neighbors":range(5, 100)
}
clf = KNeighborsClassifier()
search = GridSearchCV(clf, params, n_jobs=-1, cv=10)
search.fit(x_train_scale, y_train)
print(search.best_score_)
print(search.best_params_)

clf = KNeighborsClassifier(n_neighbors=11)
clf.fit(x_train_scale, y_train)
pre = clf.predict(x_test_scale)
df = pd.DataFrame({
    "PassengerId":test["PassengerId"],
    "Survived":pre
})
df.to_csv("knn.csv", encoding="utf-8", index=False)